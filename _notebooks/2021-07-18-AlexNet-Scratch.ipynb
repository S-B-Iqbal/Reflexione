{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet-Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dAHZGW8IP-D"
      },
      "source": [
        "# The Fish and the (Alex)Net\n",
        "\n",
        "> \"Implementation of AlexNet from scratch using PyTorch framework on a custom Dataset\"    \n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: false\n",
        "- categories: [AlexNet, PyTorch, Image Classification]\n",
        "- image: images/alexnet.png\n",
        "- hide: false\n",
        "- search_exclude: true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80vHAUsIPvn"
      },
      "source": [
        "### From Kaggle to Colab\n",
        "\n",
        "- Use the steps as devised in the [Blog](https://s-b-iqbal.github.io/Reflexione/pytorch/dataloaders/image%20manipulation/2021/06/11/Image-Loading.html) previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdUGR67mIWBs"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc_o5P2sHCsZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import time\n",
        "import seaborn as sns\n",
        "import glob\n",
        "from pathlib import Path\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VacPw-dHL_Q9"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMelpcdAHCvw"
      },
      "source": [
        "data_path = Path.cwd()/'Fish_Dataset/Fish_Dataset'\n",
        "\n",
        "# Path for all the files in a 'png' format.\n",
        "image_path = list(data_path.glob('**/*.png')) \n",
        "\n",
        "# Separate Segmented from Non-Segmented Images\n",
        "\n",
        "non_segmented_images = [img for img in image_path if 'GT' not in str(img)]\n",
        "labels_non_segment = [img.parts[-3] for img in non_segmented_images]\n",
        "\n",
        "segmented_images = [img for img in image_path if 'GT' in str(img)]\n",
        "lables_segment = [img.parts[-3] for img in segmented_images]\n",
        "\n",
        "classes = list(set(lables_segment))\n",
        "\n",
        "# Convert String Labels to int\n",
        "\n",
        "int_classes = {fish:i for i,fish in enumerate(classes)}\n",
        "\n",
        "lables = [int_classes[lable] for lable in labels_non_segment]\n",
        "\n",
        "image_data = pd.DataFrame({'Path': non_segmented_images,\\\n",
        "              'labels': lables})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmxbdD1zHCzA"
      },
      "source": [
        "train,test, train_labels, test_labels = train_test_split(image_data.Path, image_data.labels, test_size=0.2, shuffle=True)\n",
        "\n",
        "train,val, train_labels, val_labels = train_test_split(train, train_labels, test_size=0.2, shuffle=True)\n",
        "\n",
        "class FishDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Loads Images using pillow and applies transformations.\n",
        "  \"\"\"\n",
        "  def __init__(self, images, labels, transform = None):\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(self.images.iloc[idx])\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    label = self.labels.iloc[idx]\n",
        "    return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VJBceb6L6wO"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyZR0Q_qHC2c"
      },
      "source": [
        "##########################\n",
        "### FISH DATASET\n",
        "##########################\n",
        "\n",
        "# Transforming the Training Data\n",
        "train_transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomVerticalFlip(),\n",
        "                                      transforms.ColorJitter(brightness=0, contrast=0, saturation=0,hue=0.5),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                      ])\n",
        "\n",
        "# Transforming Test Data\n",
        "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def get_loaders(train, train_labels, val, val_labels,test, test_labels, batch_size, num_workers, train_transform, test_transform):\n",
        "  \"\"\"\n",
        "  Returns Train,Validation and Test Loaders.\n",
        "  \"\"\"\n",
        "\n",
        "  train_ds = FishDataset(images = train, labels = train_labels, transform = train_transform)\n",
        "  val_ds = FishDataset(images = val, labels = val_labels, transform = test_transforms)\n",
        "  test_ds = FishDataset(images = test, labels = test_labels, transform = test_transforms)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size=batch_size,num_workers=num_workers,\n",
        "                            shuffle= True)\n",
        "  val_loader = DataLoader(val_ds, batch_size=batch_size,num_workers=num_workers,\n",
        "                            shuffle= False)\n",
        "  test_loader = DataLoader(test_ds, batch_size=batch_size,num_workers=num_workers,\n",
        "                          shuffle= False)\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "  os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "  \"\"\"Compute Accuracy for the provided Data Loader\"\"\"\n",
        "  model.eval\n",
        "  with torch.no_grad():\n",
        "    correct_pred, num_examples = 0, 0\n",
        "\n",
        "    for i, (features, targets) in enumerate(data_loader):\n",
        "      features = features.to(device)\n",
        "      targets = targets.float().to(device)\n",
        "\n",
        "      logits = model(features)\n",
        "      _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "      num_examples += targets.size(0)\n",
        "      correct_pred += (predicted_labels == targets).sum()\n",
        "  return correct_pred.float()/num_examples * 100\n",
        "\n",
        "\n",
        "class UnNormalize(object):\n",
        "  \"\"\"De-Normalize Test Images, if any Normalization was done\"\"\"\n",
        "  def __init__(self, mean, std):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    for t, m, s in zip(tensor, self.mean, self.std):\n",
        "      t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "def plot_training_loss(minibatch_loss_list, num_epochs, iter_per_epoch,\n",
        "                       results_dir=None, averaging_iterations=100):\n",
        "  \"\"\"Observe the Training Loss\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  ax1 = plt.subplot(1, 1, 1)\n",
        "  ax1.plot(range(len(minibatch_loss_list)),(minibatch_loss_list), label='Minibatch Loss')\n",
        "\n",
        "  if len(minibatch_loss_list) > 1000:\n",
        "    ax1.set_ylim([0, np.max(minibatch_loss_list[1000:])*1.5])\n",
        "    ax1.set_xlabel('Iterations')\n",
        "    ax1.set_ylabel('Loss')\n",
        "\n",
        "    ax1.plot(np.convolve(minibatch_loss_list,\n",
        "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
        "                         mode='valid'),\n",
        "             label='Running Average')\n",
        "    ax1.legend()\n",
        "\n",
        "    ###################\n",
        "    # Set second x-axis\n",
        "    ###################\n",
        "    ax2 = ax1.twiny()\n",
        "    newlabel = list(range(num_epochs+1))\n",
        "\n",
        "    newpos = [e*iter_per_epoch for e in newlabel]\n",
        "\n",
        "    ax2.set_xticks(newpos[::10])\n",
        "    ax2.set_xticklabels(newlabel[::10])\n",
        "\n",
        "    ax2.xaxis.set_ticks_position('bottom')\n",
        "    ax2.xaxis.set_label_position('bottom')\n",
        "    ax2.spines['bottom'].set_position(('outward', 45))\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_xlim(ax1.get_xlim())\n",
        "    ###################\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if results_dir is not None:\n",
        "        image_path = os.path.join(results_dir, 'plot_training_loss.pdf')\n",
        "        plt.savefig(image_path)\n",
        "\n",
        "\n",
        "def plot_accuracy(train_acc_list, valid_acc_list, results_dir):\n",
        "\n",
        "    num_epochs = len(train_acc_list)\n",
        "\n",
        "    plt.plot(np.arange(1, num_epochs+1),\n",
        "             train_acc_list, label='Training')\n",
        "    plt.plot(np.arange(1, num_epochs+1),\n",
        "             valid_acc_list, label='Validation')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if results_dir is not None:\n",
        "        image_path = os.path.join(\n",
        "            results_dir, 'plot_acc_training_validation.pdf')\n",
        "        plt.savefig(image_path)\n",
        "\n",
        "\n",
        "def show_examples(model, data_loader, unnormalizer=None, class_dict=None):\n",
        "  \"\"\"Visualize the predictions\"\"\"\n",
        "  for batch_idx, (features, targets) in enumerate(data_loader):\n",
        "    with torch.no_grad():\n",
        "      features = features\n",
        "      targets = targets\n",
        "      logits = model(features)\n",
        "      predictions = torch.argmax(logits, dim=1)\n",
        "      break\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=3, ncols=5,\n",
        "                           sharex=True, sharey=True)\n",
        "    \n",
        "  if unnormalizer is not None:\n",
        "    for idx in range(features.shape[0]):\n",
        "      features[idx] = unnormalizer(features[idx])\n",
        "  nhwc_img = np.transpose(features, axes=(0, 2, 3, 1))\n",
        "    \n",
        "  if nhwc_img.shape[-1] == 1:\n",
        "    nhw_img = np.squeeze(nhwc_img.numpy(), axis=3)\n",
        "\n",
        "    for idx, ax in enumerate(axes.ravel()):\n",
        "      ax.imshow(nhw_img[idx], cmap='binary')\n",
        "      if class_dict is not None:\n",
        "        ax.title.set_text(f'P: {class_dict[predictions[idx].item()]}'\n",
        "        f'\\nT: {class_dict[targets[idx].item()]}')\n",
        "      else:\n",
        "        ax.title.set_text(f'P: {predictions[idx]} | T: {targets[idx]}')\n",
        "        ax.axison = False\n",
        "\n",
        "  else:\n",
        "    for idx, ax in enumerate(axes.ravel()):\n",
        "      ax.imshow(nhwc_img[idx])\n",
        "      if class_dict is not None:\n",
        "        ax.title.set_text(f'P: {class_dict[predictions[idx].item()]}'\n",
        "        f'\\nT: {class_dict[targets[idx].item()]}')\n",
        "      else:\n",
        "        ax.title.set_text(f'P: {predictions[idx]} | T: {targets[idx]}')\n",
        "        ax.axison = False\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYZlmAhlINUn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itIGT-k6IqCZ"
      },
      "source": [
        "##########################\n",
        "### SETTINGS\n",
        "##########################\n",
        "\n",
        "RANDOM_SEED = 123\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "WORKERS = 2\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrrA9TQWI3bW"
      },
      "source": [
        "set_all_seeds(RANDOM_SEED)\n",
        "\n",
        "train_loader, val_loader, test_loader = get_loaders(train,train_labels,val, val_labels, test,test_labels, BATCH_SIZE,WORKERS,\n",
        "                                                    train_transform, test_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRTMlAQrI88M"
      },
      "source": [
        "## AlexNet\n",
        "----    \n",
        "\n",
        "One of the most grounbreaking events in the DL community is the advent of AlexNet that was head and shoulders ahead of every other conceivable solution for _ImageNet 2012 Challenge_    \n",
        "\n",
        "AlexNet is a variant of CNN which because of it's success in the _ImageNet 2012 Challenge_ showcased the ability of neural nets in beating manually crafted models. \n",
        "\n",
        "\n",
        "### Architecture\n",
        "\n",
        "![](http://d2l.ai/_images/alexnet.svg)    \n",
        "*Comparison of the architectures of LeNet and AlexNet by CMG Lee using data from http://d2l.ai/chapter_convolutional-neural-networks/lenet.html and http://d2l.ai/chapter_convolutional-modern/alexnet.html .*    \n",
        "\n",
        "- One can observe that while LeNet-5 is deep with 5 layers(2 Convolutions and 3 fully connected layers), AlexNet is much deeper with 8 layers(5 Convolution Layers and 3 fully connected ones). From an architectural point of view, both the networks are similar.\n",
        "\n",
        "- AlexNet resorts to ReLU as an activation function whereas LeNet used sigmoid. \n",
        "\n",
        "- In _LeNet_ we were striding using the same size as that of Kernel.\n",
        "\n",
        "- The AlexNet architecture also employed __dropout__ as a regularization technique whereas LeNet relied on weight decay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iwjp1kfI6Fl"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Implementation of AlexNet, from paper\n",
        "  \"ImageNet Classification with Deep Convolutional Neural Networks\" by Alex Krizhevsky et al.\n",
        "  See: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
        "  \"\"\"\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        # Here, we use a larger 11 x 11 window to capture objects. At the same\n",
        "        # time, we use a stride of 4 to greatly reduce the height and width of the\n",
        "        # output. Here, the number of output channels is much larger than that in\n",
        "        # LeNet\n",
        "        nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        # Make the convolution window smaller, set padding to 2 for consistent\n",
        "        # height and width across the input and output, and increase the number of\n",
        "        # output channels\n",
        "        nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        # Use three successive convolutional layers and a smaller convolution\n",
        "        # window. Except for the final convolutional layer, the number of output\n",
        "        # channels is further increased. Pooling layers are not used to reduce the\n",
        "        # height and width of input after the first two convolutional layers\n",
        "        nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
        "        # Here, the number of outputs of the fully-connected layer is several\n",
        "        # times larger than that in LeNet. Use the dropout layer to mitigate\n",
        "        # overfitting\n",
        "        nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
        "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
        "        # Output layer. Since we are using Fashion-MNIST, the number of classes is\n",
        "        # 10, instead of 1000 as in the paper\n",
        "        nn.Linear(4096, num_classes)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    logits = self.layers(x)\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io-Y7AqjKmYg"
      },
      "source": [
        "### Observing the Network\n",
        "\n",
        "- A tri-channel example datapoint is created to observe the shape of the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsZfhCMPKocA",
        "outputId": "7fb0ea1a-edc6-496c-8a44-e279300d86e1"
      },
      "source": [
        "X = torch.randn(1,3,224,224)\n",
        "\n",
        "net = AlexNet(num_classes=9)\n",
        "for layer in net.layers:\n",
        "  X = layer(X)\n",
        "  print(layer.__class__.__name__,f\"\\t output shape: {X.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conv2d \t output shape: torch.Size([1, 96, 54, 54])\n",
            "ReLU \t output shape: torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d \t output shape: torch.Size([1, 96, 26, 26])\n",
            "Conv2d \t output shape: torch.Size([1, 256, 26, 26])\n",
            "ReLU \t output shape: torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d \t output shape: torch.Size([1, 256, 12, 12])\n",
            "Conv2d \t output shape: torch.Size([1, 384, 12, 12])\n",
            "ReLU \t output shape: torch.Size([1, 384, 12, 12])\n",
            "Conv2d \t output shape: torch.Size([1, 384, 12, 12])\n",
            "ReLU \t output shape: torch.Size([1, 384, 12, 12])\n",
            "Conv2d \t output shape: torch.Size([1, 256, 12, 12])\n",
            "ReLU \t output shape: torch.Size([1, 256, 12, 12])\n",
            "MaxPool2d \t output shape: torch.Size([1, 256, 5, 5])\n",
            "Flatten \t output shape: torch.Size([1, 6400])\n",
            "Linear \t output shape: torch.Size([1, 4096])\n",
            "ReLU \t output shape: torch.Size([1, 4096])\n",
            "Dropout \t output shape: torch.Size([1, 4096])\n",
            "Linear \t output shape: torch.Size([1, 4096])\n",
            "ReLU \t output shape: torch.Size([1, 4096])\n",
            "Dropout \t output shape: torch.Size([1, 4096])\n",
            "Linear \t output shape: torch.Size([1, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahF3kx3LKDEn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYJ3vVAwKGVM"
      },
      "source": [
        "model = AlexNet(num_classes=9)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n",
        "# Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       factor=0.1,\n",
        "                                                       mode='max',\n",
        "                                                       verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFUlC1oGsg7U"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K-3LiRFKOln",
        "outputId": "eaa0d5e3-98b7-46ad-fd2c-14da49cc2cd0"
      },
      "source": [
        "logging_interval = 50\n",
        "scheduler_on='minibatch_loss'\n",
        "start_time = time.time()\n",
        "\n",
        "minibatch_loss_list, train_acc_list, valid_acc_list = [],[],[]\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  # Start Training\n",
        "  model.train()\n",
        "  for batch_idx, (features, target) in enumerate(train_loader):\n",
        "    features = features.to(DEVICE)\n",
        "    targets = target.to(DEVICE)\n",
        "    # Forward and BackPropagation\n",
        "    logits = model(features)\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update Model Parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    ## LOGGING\n",
        "    minibatch_loss_list.append(loss.item())\n",
        "    if not batch_idx % logging_interval:\n",
        "      print(f\"Epoch = {epoch+1:03d}/{NUM_EPOCHS:03d}\"\n",
        "      f\"| Batch {batch_idx:04d}/{len(train_loader):04d}\"\n",
        "      f\"| Loss: {loss:.4f}\")\n",
        "    \n",
        "  ## Validation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
        "    valid_acc = compute_accuracy(model, val_loader, DEVICE)\n",
        "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS:03d} '\n",
        "    f'| Train: {train_acc :.2f}% '\n",
        "    f'| Validation: {valid_acc :.2f}%')\n",
        "    train_acc_list.append(train_acc)\n",
        "    valid_acc_list.append(valid_acc)\n",
        "    \n",
        "  elapsed = (time.time() - start_time)/60\n",
        "  print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "  if scheduler is not None:\n",
        "    if scheduler_on == \"valid_acc\":\n",
        "      scheduler.step(valid_acc_list[-1])\n",
        "    if scheduler_on == 'minibatch_loss':\n",
        "      scheduler.step(minibatch_loss_list[-1])\n",
        "    else:\n",
        "      raise ValueError(\"Invalid `scheduler_on` choice\")\n",
        "\n",
        "total_elapsed = (time.time() - start_time)/60\n",
        "print(f'Total Training Time: {total_elapsed:.2f} min')\n",
        "\n",
        "# Compute Test Accuracy\n",
        "\n",
        "test_acc = compute_accuracy(model, test_loader, device=DEVICE)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc:0.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch = 001/010| Batch 0000/0090| Loss: 2.1966\n",
            "Epoch = 001/010| Batch 0050/0090| Loss: 2.1958\n",
            "Epoch: 1/010 | Train: 11.25% | Validation: 10.07%\n",
            "Time elapsed: 3.17 min\n",
            "Epoch = 002/010| Batch 0000/0090| Loss: 2.1973\n",
            "Epoch = 002/010| Batch 0050/0090| Loss: 2.1971\n",
            "Epoch: 2/010 | Train: 11.25% | Validation: 10.07%\n",
            "Time elapsed: 6.35 min\n",
            "Epoch = 003/010| Batch 0000/0090| Loss: 2.1957\n",
            "Epoch = 003/010| Batch 0050/0090| Loss: 2.1996\n",
            "Epoch: 3/010 | Train: 13.82% | Validation: 13.40%\n",
            "Time elapsed: 9.57 min\n",
            "Epoch = 004/010| Batch 0000/0090| Loss: 2.1974\n",
            "Epoch = 004/010| Batch 0050/0090| Loss: 2.1993\n",
            "Epoch: 4/010 | Train: 20.57% | Validation: 17.99%\n",
            "Time elapsed: 12.76 min\n",
            "Epoch = 005/010| Batch 0000/0090| Loss: 2.1964\n",
            "Epoch = 005/010| Batch 0050/0090| Loss: 2.1964\n",
            "Epoch: 5/010 | Train: 15.82% | Validation: 14.65%\n",
            "Time elapsed: 15.92 min\n",
            "Epoch = 006/010| Batch 0000/0090| Loss: 2.1969\n",
            "Epoch = 006/010| Batch 0050/0090| Loss: 2.1958\n",
            "Epoch: 6/010 | Train: 11.56% | Validation: 9.44%\n",
            "Time elapsed: 19.13 min\n",
            "Epoch = 007/010| Batch 0000/0090| Loss: 2.1962\n",
            "Epoch = 007/010| Batch 0050/0090| Loss: 2.2024\n",
            "Epoch: 7/010 | Train: 11.56% | Validation: 9.44%\n",
            "Time elapsed: 22.32 min\n",
            "Epoch = 008/010| Batch 0000/0090| Loss: 2.1971\n",
            "Epoch = 008/010| Batch 0050/0090| Loss: 2.1966\n",
            "Epoch: 8/010 | Train: 11.56% | Validation: 9.44%\n",
            "Time elapsed: 25.55 min\n",
            "Epoch = 009/010| Batch 0000/0090| Loss: 2.1971\n",
            "Epoch = 009/010| Batch 0050/0090| Loss: 2.1996\n",
            "Epoch: 9/010 | Train: 11.56% | Validation: 9.44%\n",
            "Time elapsed: 28.82 min\n",
            "Epoch = 010/010| Batch 0000/0090| Loss: 2.1947\n",
            "Epoch = 010/010| Batch 0050/0090| Loss: 2.1927\n",
            "Epoch: 10/010 | Train: 11.56% | Validation: 9.44%\n",
            "Time elapsed: 32.02 min\n",
            "Total Training Time: 1921.41 min\n",
            "Test accuracy: 11.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LUhJL3EY20x"
      },
      "source": [
        "- We covered steps for constructing a complete DL Pipeline i.e., from fetching the data to using the Model for prediction on unseen Data.    \n",
        "- In short, Test accuracy is **11 %**\n",
        "- Training TIme is approx **32 Mins**    \n",
        "- Well, not so great results on our custom Dataset. Nonetheless, AlexNet was specifically designet for ImageNet challenge.\n",
        "\n",
        "- One of the very first examples of Deep Neural Nets after which we saw how important architecture was in increasing the accuracy of the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3L_MXFjyBPL"
      },
      "source": [
        "### References:\n",
        "\n",
        "1. Code:\n",
        "----\n",
        "- [Sebastian Raschka](https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb)'s code in a plug-and-play format was superhelpful.\n",
        "\n",
        "2. Architecture:\n",
        "----\n",
        "[ImageNet Classification with Deep ConvolutionalNeural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) by Alex et al.\n"
      ]
    }
  ]
}